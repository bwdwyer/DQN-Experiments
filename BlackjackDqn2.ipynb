{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-02T04:38:29.789078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.3\n",
      "Num GPUs Available:  0\n",
      "Episode 0 | epsilon 1.0\n",
      "Episode 100 | epsilon 0.6057704364907278\n",
      "Episode 200 | epsilon 0.3669578217261671\n",
      "Episode 300 | epsilon 0.22229219984074702\n",
      "Episode 400 | epsilon 0.1346580429260134\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "print(pd.__version__)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Create the environment\n",
    "# Blackjack env source: https://github.com/sjchoi86/advanced-tensorflow/blob/master/rl/env/blackjack.py\n",
    "tf.keras.backend.clear_session()\n",
    "env = gym.make('Blackjack-v1')\n",
    "\n",
    "# Define the neural network\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # tf.keras.layers.Dense(32, activation='relu', input_shape=(3,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu', input_shape=(3,)),\n",
    "        # tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(env.action_space.n)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the epsilon greedy strategy\n",
    "def get_action(model, state, epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(env.action_space.n)\n",
    "    return np.argmax(model.predict(state, verbose=0))\n",
    "\n",
    "\n",
    "# Define the replay memory\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        samples = random.choices(self.buffer, k=sample_size)\n",
    "        return map(list, zip(*samples))\n",
    "\n",
    "\n",
    "class BatchLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.batch_logs = []\n",
    "        self.total_batches_seen = 0\n",
    "        self.log_df = None\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.total_batches_seen += 1\n",
    "        self.batch_logs.append({'Iteration': self.total_batches_seen, 'Loss': logs['loss'], 'Accuracy': logs['accuracy']})\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Create the log DataFrame at the end of each epoch\n",
    "        self.log_df = pd.DataFrame(self.batch_logs)\n",
    "logger = BatchLogger()\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 1\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.1\n",
    "episodes = 3_000  # 2_000\n",
    "batch_size = 2 ** 16 #  8192  # 32\n",
    "gamma = 1  # 0.95\n",
    "replay_buffer = ReplayBuffer(10_000)\n",
    "\n",
    "\n",
    "# Create the DQN model\n",
    "model = create_model()\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()  # Get the current time\n",
    "for e in range(episodes):\n",
    "    if e % 100 == 0:\n",
    "        print(f\"Episode {e} | epsilon {epsilon}\")\n",
    "\n",
    "    state = np.reshape(env.reset(), [1, 3])\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = get_action(model, state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, 3])\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "    # Replay\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    states = np.squeeze(np.array(states), axis=1)\n",
    "    targets = model.predict(np.array(states), verbose=0)\n",
    "\n",
    "    next_states = np.squeeze(np.array(next_states), axis=1)\n",
    "    next_q_values = model.predict(np.array(next_states), verbose=0)\n",
    "\n",
    "    for i, done in enumerate(dones):\n",
    "        if done:\n",
    "            targets[i][actions[i]] = rewards[i]\n",
    "        else:\n",
    "            targets[i][actions[i]] = rewards[i] + gamma * np.amax(next_q_values[i])\n",
    "    model.fit(np.array(states), targets, epochs=epochs, verbose=0, callbacks=[logger],)\n",
    "\n",
    "    # Adjust epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "\n",
    "end_time = time.time()  # Get the current time again after your code has run\n",
    "execution_time = end_time - start_time  # Calculate the difference\n",
    "print(f\"The execution time was: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(logger.log_df['Iteration'], logger.log_df['Loss'])\n",
    "plt.title('Loss over iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(logger.log_df['Iteration'], logger.log_df['Accuracy'])\n",
    "plt.title('Accuracy over iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "basic_strategy_hard_grid = [\n",
    "    # Dealer Upcard\n",
    "    # 2, 3, 4, 5, 6, 7, 8, 9, 10, A\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *4*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *5*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *6*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *7*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *8*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *9*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *10*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *11*#\n",
    "    [1, 1, 0, 0, 0, 1, 1, 1, 1, 1, ],  # *12*#\n",
    "    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ],  # *13*#\n",
    "    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ],  # *14*#\n",
    "    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ],  # *15*#\n",
    "    [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ],  # *16*#\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],  # *17*#\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],  # *18*#\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],  # *19*#\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],  # *20*#\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],  # *21*#\n",
    "]\n",
    "\n",
    "basic_strategy_soft_grid = [\n",
    "    # Dealer Upcard\n",
    "    # 2, 3, 4, 5, 6, 7, 8, 9, 10, A\n",
    "    # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *4*#\n",
    "    # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *5*#\n",
    "    # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *6*#\n",
    "    # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *7*#\n",
    "    # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *8*#\n",
    "    # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *9*#\n",
    "    # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *10*#\n",
    "    # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *11*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *12*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *13*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *14*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *15*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *16*#\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ],  # *17*#\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ],  # *18*#\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],  # *19*#\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],  # *20*#\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ],  # *21*#\n",
    "]\n",
    "\n",
    "\n",
    "def basic_strategy(player_hand, dealer_upcard, usable_ace):\n",
    "    \"\"\"\n",
    "    Returns the ideal play for the player given a blackjack game state.\n",
    "\n",
    "    Args:\n",
    "        player_hand (int): The value of the player's hand.\n",
    "        dealer_upcard (int): The value of the dealer's up-card.\n",
    "        usable_ace (bool): True if the player has a soft ace, False otherwise.\n",
    "\n",
    "    Returns:\n",
    "        int: The recommended play for the player. 0 for 'stand', 1 for 'hit'.\n",
    "    \"\"\"\n",
    "\n",
    "    if (not usable_ace and player_hand < 4) or (usable_ace and player_hand < 12) or player_hand > 21:\n",
    "        raise ValueError(f\"Invalid player hand value {player_hand}. Must be between 4 and 21.\")\n",
    "\n",
    "    if dealer_upcard < 1 or dealer_upcard > 10:\n",
    "        raise ValueError(f\"Invalid dealer up-card value {dealer_upcard}. Must be between 2 and 11.\")\n",
    "\n",
    "    i_player_hand = player_hand - 12 if usable_ace else player_hand - 4\n",
    "    i_dealer_upcard = 9 if dealer_upcard == 1 else dealer_upcard - 2\n",
    "\n",
    "    # print(f\"{usable_ace} | {player_hand}-{i_player_hand} | {dealer_upcard}-{i_dealer_upcard}\")\n",
    "    if usable_ace:\n",
    "        return basic_strategy_soft_grid[i_player_hand][i_dealer_upcard]\n",
    "    else:\n",
    "        return basic_strategy_hard_grid[i_player_hand][i_dealer_upcard]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# Create two grids to hold the results\n",
    "# Player sum can range from 12 to 21 inclusive when ace is usable, hence the size is 10\n",
    "# Player sum can range from 4 to 21 inclusive when no ace is usable, hence the size is 18\n",
    "grid_usable_ace = np.zeros((10, 10))\n",
    "grid_no_usable_ace = np.zeros((18, 10))\n",
    "\n",
    "num_mismatches = 0\n",
    "total_cases = 0\n",
    "\n",
    "# Loop over player sums, dealer cards, and usable ace\n",
    "for dealer_card in range(1, 11):  # Dealer card can be from 1 to 10\n",
    "    for usable_ace in [False, True]:  # Usable ace can be True or False\n",
    "        # Player sum starts from 12 when ace is usable, 4 otherwise\n",
    "        for player_sum in range(12 if usable_ace else 4, 22):  # Player sum can be from 4/12 to 21\n",
    "\n",
    "            # Create the state\n",
    "            state = np.array([[player_sum, dealer_card, int(usable_ace)]])\n",
    "\n",
    "            # Get the actions chosen by the DQN model and the basic strategy\n",
    "            dqn_action = np.argmax(model.predict(state, verbose=0))\n",
    "            basic_strategy_action = basic_strategy(player_sum, dealer_card, usable_ace)\n",
    "\n",
    "            # Compare the actions\n",
    "            if dqn_action != basic_strategy_action:\n",
    "                num_mismatches += 1\n",
    "                # Mark the cell in the appropriate grid\n",
    "                if usable_ace:\n",
    "                    grid_usable_ace[player_sum - 12, dealer_card - 1] = 2 if basic_strategy_action == 1 else -2\n",
    "                else:\n",
    "                    grid_no_usable_ace[player_sum - 4, dealer_card - 1] = 2 if basic_strategy_action == 1 else -2\n",
    "            else:\n",
    "                if usable_ace:\n",
    "                    grid_usable_ace[player_sum - 12, dealer_card - 1] = 0 if basic_strategy_action == 1 else -1\n",
    "                else:\n",
    "                    grid_no_usable_ace[player_sum - 4, dealer_card - 1] = 0 if basic_strategy_action == 1 else -1\n",
    "\n",
    "            total_cases += 1\n",
    "\n",
    "print(\"Total mismatches:\", num_mismatches)\n",
    "print(\"Total cases:\", total_cases)\n",
    "print(\"Mismatch percentage:\", num_mismatches / total_cases * 100)\n",
    "\n",
    "# Create a color map to distinguish between 'stand' (-1), 'hit' (1), \n",
    "# mismatch where the basic strategy says to 'stand' (-2), \n",
    "# and mismatch where the basic strategy says to 'hit' (2)\n",
    "cmap = colors.ListedColormap(['red', 'blue', 'green', 'yellow'])\n",
    "bounds = [-2.5, -1.5, -0.5, 0.5, 1.5]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# Plot the grids\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "im = ax[0].imshow(grid_usable_ace, cmap=cmap, norm=norm, interpolation='nearest')\n",
    "ax[0].set_title('Usable Ace')\n",
    "ax[0].set_xticks(range(10))\n",
    "ax[0].set_xticklabels(range(1, 11))\n",
    "ax[0].set_yticks(range(10))\n",
    "ax[0].set_yticklabels(range(12, 22))\n",
    "ax[0].set_xlabel('Dealer Card')\n",
    "ax[0].set_ylabel('Player Sum')\n",
    "\n",
    "im = ax[1].imshow(grid_no_usable_ace, cmap=cmap, norm=norm, interpolation='nearest')\n",
    "ax[1].set_title('No Usable Ace')\n",
    "ax[1].set_xticks(range(10))\n",
    "ax[1].set_xticklabels(range(1, 11))\n",
    "ax[1].set_yticks(range(18))\n",
    "ax[1].set_yticklabels(range(4, 22))\n",
    "ax[1].set_xlabel('Dealer Card')\n",
    "ax[1].set_ylabel('Player Sum')\n",
    "\n",
    "# Add a color bar\n",
    "cbar = fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "cbar.set_ticks([-1.5, -0.5, 0.5, 1.5])\n",
    "cbar.set_ticklabels(['Mismatch (Stand)', 'Stand', 'Hit', 'Mismatch (Hit)'])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "np.argmax(model.predict([[12, 5, int(False)]], verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "basic_strategy(12, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
